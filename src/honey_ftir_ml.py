# -*- coding: utf-8 -*-
"""honey_FTIR_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13s5b1bwWNW70yUHjS01BSj8JNqP07aCF
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, f1_score


# To make the results consistent in multiple runs
np.random.seed(42)

# To plot figures
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize= 14)
mpl.rc('xtick', labelsize= 12)
mpl.rc('ytick', labelsize= 12)

# Where to save figures
def save_fig(fig, filename, folder_path='MyDrive/HoneyFTIR/Figures', dpi=300):
    """
    Parameters:
    - fig: matplotlib.figure.Figure object (e.g., from plt.gcf())
    - filename: string, name of the file (e.g., 'plot.png')
    - folder_path: string, path within MyDrive where the file will be saved
    - dpi: int, resolution in dots per inch
    """
    # Build full path
    full_path = os.path.join('/content/drive', folder_path)
    os.makedirs(full_path, exist_ok=True)  # Create folder if it doesn't exist
    save_path = os.path.join(full_path, filename)

    # Save the figure
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight')
    print(f"Figure saved to: {save_path}")

IMAGE_PATH = '/content/drive/MyDrive/HoneyFTIR/Figures/'

# Connect with google drive
import os
from google.colab import drive
import glob

# # Attempt to remove the mount point directory if it exists and is not empty
# if os.path.exists('/content/drive') and os.path.isdir('/content/drive'):
#     try:
#         os.system('rm -rf /content/drive/*')
#     except OSError as e:
#         print(f"Error removing existing files: {e}")

drive.mount('/content/drive', force_remount=True)

# Load data
folder_path = '/content/drive/MyDrive/HoneyFTIR'
files = glob.glob(os.path.join(folder_path, '*.xls'))

print(files)

# Wavenumber axis
n_points = 1789
wavenumbers = np.arange(424, 424 + 2*n_points, 2)  # 4000 -> 424 cm^-1

# Function to read excel files and reshape
def process_honey_file(file_path, skiprows= 15):
  """
  Read Excel files and convert them into ML-ready format

  Returns dataframe where columns= wavenumbers, plus 'sample_id', and 'class'
  """
  class_name = os.path.splitext(os.path.basename(file_path))[0]
  df = pd.read_excel(file_path, skiprows= skiprows, header= 0) # row 16 - header

  # Transpose so that rows= spectra, columns= absorbance
  df_T = df.T.reset_index()
  df_T.rename(columns= {'index' : 'sample_id'}, inplace= True)

  # Keep only absorbance columns and rename them to wavenumbers
  df_T = df_T.iloc[:, :n_points+1]  # +1 for sample_id
  df_T.columns = ['sample_id'] + list(wavenumbers)

  # Add label
  df_T['class'] = class_name

  return df_T

all_data = []

for file in files:
  df_processed = process_honey_file(file, skiprows= 15)
  all_data.append(df_processed)

dataset = pd.concat(all_data, ignore_index= True)
dataset.set_index('sample_id', inplace= True)

dataset.to_excel("HoneyFTIR_merged.xlsx", index= False)

from google.colab import files
files.download("HoneyFTIR_merged.xlsx")

print('Shape: ', dataset.shape)
print("Classes: ", dataset['class'].value_counts())

"""# Libraries"""

import copy
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# metrics
from sklearn.metrics import accuracy_score, f1_score

# le = LabelEncoder()
# dataset['code'] = le.fit_transform(dataset['class'])

# X  = dataset.drop(columns= ['class', 'code'])
# y = dataset['code']

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y,
#     test_size= 0.2,
#     random_state= 42,
#     stratify= y
# )

# model = SVC(random_state= 42)
# model.fit(X_train, y_train)

# # Eval
# y_pred = model.predict(X_test)
# accuracy = accuracy_score(y_test, y_pred)

# accuracy

best_combination = None
best_accuracy = 0

# Choosing 5 classes from the combination of 7 classes
import itertools

# Generate all classes combination
class_counts = dataset['class'].value_counts()
top8_classes = class_counts.head(8).index.tolist()
combinations = list(itertools.combinations(top8_classes, 5))

# Iterate through each combination
for combination in combinations:
  mask = dataset['class'].isin(combination)
  filtered_data = copy.deepcopy(dataset[mask])

  # Code the labels into numerals
  le = LabelEncoder()
  filtered_data['Botanic Code'] = le.fit_transform(filtered_data['class'])


  # Split features and label
  X = filtered_data.drop(columns= ['class', 'Botanic Code'])
  y = filtered_data['Botanic Code']

  # Convert column names to strings
  X.columns = X.columns.astype(str)

  # Scale features
  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  # Split training set and test set
  X_train, X_test, y_train, y_test = train_test_split(
      X, y,
      test_size= 0.2,
      random_state= 42,
      stratify= y
  )

  # Create model
  model = SVC(class_weight= 'balanced', random_state= 42)
  model.fit(X_train, y_train)

  # Eval
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)

  # Keep best score
  if best_combination == None or accuracy > best_accuracy:
    best_combination = combination
    best_accuracy = accuracy

  # Print
  print("Best combination so far: ", best_combination)
  print("Best accuracy so far: ", best_accuracy)

  # Delete the model
  del model

best_combination = ('polyfloral', 'fir honeydew', 'chestnut', 'acacia', 'rape')

mask = dataset['class'].isin(best_combination)
filtered_data = copy.deepcopy(dataset[mask])

filtered_data['class'].value_counts()

# import itertools
# import copy
# import pandas as pd
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.model_selection import train_test_split
# from sklearn.svm import SVC
# from sklearn.metrics import accuracy_score

# # Count top 8 classes
# class_counts = dataset['class'].value_counts()
# top8_classes = class_counts.head(8).index.tolist()

# # Variables to keep track of the best combination
# best_combination_overall = None
# best_accuracy_overall = 0

# # DataFrame to store all results
# results = pd.DataFrame(columns=['combination', 'num_classes', 'accuracy'])

# # Loop over combination sizes 4 to 8
# for r in range(4, 9):  # 4,5,6,7,8
#     combinations = list(itertools.combinations(top8_classes, r))

#     for combination in combinations:
#         mask = dataset['class'].isin(combination)
#         filtered_data = copy.deepcopy(dataset[mask])

#         # Encode class labels
#         le = LabelEncoder()
#         filtered_data['Botanic Code'] = le.fit_transform(filtered_data['class'])

#         # Split features and label
#         X = filtered_data.drop(columns=['class', 'Botanic Code', 'sample_id'], errors='ignore')
#         y = filtered_data['Botanic Code']

#         # Ensure columns are strings
#         X.columns = X.columns.astype(str)

#         # Scale features
#         scaler = StandardScaler()
#         X_scaled = scaler.fit_transform(X)

#         # Split into train/test
#         X_train, X_test, y_train, y_test = train_test_split(
#             X_scaled, y,
#             test_size=0.2,
#             random_state=42,
#             stratify=y
#         )

#         # Train model
#         model = SVC(class_weight='balanced', random_state=42)
#         model.fit(X_train, y_train)

#         # Evaluate
#         y_pred = model.predict(X_test)
#         accuracy = accuracy_score(y_test, y_pred)

#         # Update best combination if needed
#         if accuracy > best_accuracy_overall:
#             best_combination_overall = combination
#             best_accuracy_overall = accuracy

#         # Save current result
#         results = pd.concat([results, pd.DataFrame({
#             'combination': [combination],
#             'num_classes': [r],
#             'accuracy': [accuracy]
#         })], ignore_index=True)

#         del model

# # Print final best result
# print("✅ Best combination overall:", best_combination_overall)
# print("✅ Best accuracy overall:", best_accuracy_overall)

# # Optional: save results to CSV

# results_sorted = results.sort_values(by= 'accuracy', ascending= False).reset_index(drop= True)
# print(results_sorted.head(30))

"""# Pre-processing"""

# Code botanic name in numerical label
le = LabelEncoder()
filtered_data['Botanic Code'] = le.fit_transform(filtered_data['class'])

print('---- LabelEncoder Mapping----')
# LabelEncoder classes_ attribute stores original classes in order it transformed them (alphabetical order)
for i, class_name in enumerate(le.classes_):
  print(f'{i} : {class_name}')

# Split features and label
X = filtered_data.drop(columns= ['class', 'Botanic Code'])
y = filtered_data['Botanic Code']

print('X shape: ', X.shape)
print('y shape: ', y.shape)
print('Number of classes: ',y.nunique())

# Split the data to training and test sets
# Since the sample size of different class is significantly different
# Stratified sampling is used to for representative sampling
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size= 0.2,
    random_state= 42,
    stratify= y
)

print("X train size: ", X_train.shape)
print("y train size: ", y_train.shape)
print("X test size: ", X_test.shape)
print("y test size: ", y_test.shape)

# Standardize the data (mean centered with variance of 1)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print('mean: ', X_train_scaled.mean())
print('variance: ', X_train_scaled.var())

"""# Machine Learning Algorithms

### SVM
"""

results = []

from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from scipy.stats import reciprocal, uniform

print("---Training svm by one-vs-rest approach and using randomized search for hyperparameter tuning---")
svm_clf = SVC(gamma= 'scale', class_weight= 'balanced')
param_distributions = {'gamma' : reciprocal(0.00001, 0.1), 'C' : uniform(1, 4)}
rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter= 100, verbose= 1, cv= 3)
rnd_search_cv.fit(X_train_scaled, y_train)

svm_clf = rnd_search_cv.best_estimator_

y_pred = svm_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((svm_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-Score: ", f1)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

params = {'max_depth' : list(range(1, 25))}
tree_grid_search_cv = GridSearchCV(DecisionTreeClassifier(class_weight= 'balanced', random_state= 42), params, verbose= 1, cv= 2)
tree_grid_search_cv.fit(X_train_scaled, y_train)

tree_clf = tree_grid_search_cv.best_estimator_
tree_clf.fit(X_train_scaled, y_train)

tree_clf = DecisionTreeClassifier(class_weight= 'balanced', max_depth= 4, random_state= 42)
tree_clf.fit(X_train_scaled, y_train)

# Import export_graphviz to visualize graph definition file (.dot)
from sklearn.tree import export_graphviz
from graphviz import Source

IMAGE_PATH =  '/content/drive/MyDrive/HoneyFTIR/Figures'

# If your X is a DataFrame, get feature names as strings of wavenumbers
feature_names = [f"{col} cm$^{-1}$)" for col in X.columns]

export_graphviz(
    tree_clf,
    out_file= os.path.join(IMAGE_PATH, 'honey_classifier_decision_tree.dot'),
    feature_names=feature_names,   # use wavenumber labels instead of x[116]
    class_names=[str(name) for name in le.classes_], # optional, if you want labels instead of class IDs
    rounded= True,
    filled= True
)

graph = Source.from_file(os.path.join(IMAGE_PATH, 'honey_classifier_decision_tree.dot'))
graph.format = "png"   # can also be 'pdf'
graph.render("decision_tree")   # saves as decision_tree.png

# Import export_graphviz to visualize graph definition file (.dot)
from sklearn.tree import export_graphviz
from graphviz import Source
import os

IMAGE_PATH = '/content/drive/MyDrive/HoneyFTIR/Figures'

# If your X is a DataFrame, get feature names as strings of wavenumbers
feature_names = [f"{col} cm$^{-1}$" for col in X.columns]  # fixed unmatched parenthesis

export_graphviz(
    tree_clf,
    out_file=os.path.join(IMAGE_PATH, 'honey_classifier_decision_tree.dot'),
    feature_names=feature_names,
    class_names=[str(name) for name in le.classes_],
    rounded=True,
    filled=True,
    max_depth=3   # <-- limit visualization to first 3 levels
)

graph = Source.from_file(os.path.join(IMAGE_PATH, 'honey_classifier_decision_tree.dot'))
graph.format = "png"
graph.render(os.path.join(IMAGE_PATH, "decision_tree_limited"))  # saves as decision_tree_limited.png

y_pred = tree_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_pred, y_test, average= 'macro')
# results.append((tree_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-score: ", f1)

"""# Ensemble Learning & Random Forest"""

#  Bagging classifier
from sklearn.ensemble import BaggingClassifier

bag_clf = BaggingClassifier(DecisionTreeClassifier(class_weight= 'balanced', random_state= 42), n_estimators= 500, max_samples= 100, bootstrap= True, n_jobs= -1
)
bag_clf.fit(X_train_scaled, y_train)

y_pred = bag_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((bag_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-score: ", f1)

"""## Feature Importance"""

from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators= 1000, class_weight= 'balanced', random_state= 42)
rnd_clf.fit(X_train_scaled, y_train)

# X should be a DataFrame (not numpy array) to access column names
# Make sure X columns are your wavenumbers
feature_names = X.columns  # this will be the wavenumbers

# Get feature importances from your trained RandomForest (or similar)
importances = rnd_clf.feature_importances_
sorted_indices = np.argsort(importances)[::-1]

# Select top 10 features
top_indices = sorted_indices[:10]
top_importances = importances[top_indices]

# Print wavenumber instead of index
feature_importance = []  # list of (wavenumber, importance)

for idx, importance in zip(top_indices, importances):
    wavenumber = feature_names[idx]  # get column name
    # print(f"Wavenumber: {wavenumber}, Importance: {importance:.6f}")
    feature_importance.append((wavenumber, importance))

print(feature_importance)

# X should be a DataFrame (not numpy array) to access column names
# Make sure X columns are your wavenumbers
feature_names = X.columns  # this will be the wavenumbers

# Get feature importances from your trained RandomForest (or similar)
importances = rnd_clf.feature_importances_
sorted_indices = np.argsort(importances)[::-1]

# Select top 20 features
top_indices = sorted_indices[:20]
print(top_indices)
top_importances = importances[sorted_indices]

# Print wavenumber instead of index
feature_importance = []  # list of (wavenumber, importance)

for idx, importance in zip(sorted_indices, importances):
    wavenumber = feature_names[idx]  # get column name
    # print(f"Wavenumber: {wavenumber}, Importance: {importance:.6f}")
    feature_importance.append((wavenumber, importance))

# Plot feature importance
df_imp = pd.DataFrame(feature_importance, columns= ['wavenumber', 'importance'])
df_imp['wavenumber'] = df_imp['wavenumber'].astype(float)


# Define meaningful FTIR bins
bins = [400, 600, 1000, 1200, 1500, 1650, 1750, 2800, 3000, 3600, 4200]
labels = [
    "400–600 Fingerprint (low)",
    "600–1000 Fingerprint (mid)",
    "1000–1200 C–O / sugars",
    "1200–1500 Fingerprint (high)",
    "1500–1650 Mid-region",
    "1650–1750 C=O stretch",
    "1750–2800 Gap",
    "2800–3000 C–H stretch",
    "3000–3600 O–H stretch",
    "3600–4200 High-energy / H-bond"
]

# Bin features
df_imp['bin'] = pd.cut(df_imp['wavenumber'], bins=bins, labels=labels, right=False)

# Sum importance per bin
bin_importance = df_imp.groupby('bin')['importance'].sum().reindex(labels)
# Plot
plt.figure(figsize=(12, 6))  # wider figure
bars = plt.bar(bin_importance.index, bin_importance.values, color= 'teal')

plt.xticks(rotation=45, ha='right', fontsize=10)  # smaller font
bin_importance.plot(kind= 'bar', color= '#F7C469')
plt.ylabel("Sum of feature importance", fontsize=12, labelpad=15)  # increase space
plt.xlabel("Wavenumber range in cm^-1")
plt.title("Feature importance per IR region")

plt.ylim(0, bin_importance.max() * 1.1)

for bar in bars:
  height = bar.get_height()
  plt.annotate(f"{height:.3f}",
               xy= (bar.get_x() + bar.get_width() / 2, height),
               xytext= (0, 3),
               textcoords = "offset points",
               ha= "center", va= "bottom", fontsize= 8
               )

plt.tight_layout()
plt.savefig(os.path.join(IMAGE_PATH, 'feature_importance_bins.pdf'), dpi= 300, bbox_inches= 'tight')

plt.show()

"""Boosting"""

# Using AdaBoost (Adaptive Boosing)
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth= 8), n_estimators= 500, learning_rate= 0.1
)
ada_clf.fit(X_train_scaled, y_train)

y_pred = ada_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((ada_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-score: ", f1)

# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier

params = {'max_depth' : list(range(2, 10)), 'n_estimators' : list(range(5))}
grad_clf_grid_search_cv = GridSearchCV(
    GradientBoostingClassifier(random_state= 42),
    params,
    verbose= 2,
    cv= 3
)

grad_clf_grid_search_cv.fit(X_train_scaled, y_train)

gb_clf = grad_clf_grid_search_cv.best_estimator_

# fit gradiant boosting classifier
gb_clf.fit(X_train_scaled, y_train)

y_pred = gb_clf.predict(X_test_scaled)
accuracy  = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((gb_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-score: ", f1)

best_depth = grad_clf_grid_search_cv.best_estimator_.max_depth

# Optimal number of trees to ensemble
from sklearn.metrics import mean_squared_error

gbc = GradientBoostingClassifier(max_depth= best_depth, n_estimators= 3)
gbc.fit(X_train_scaled, y_train)

errors = [mean_squared_error(y_test, y_pred) for y_pred in gbc.staged_predict(X_test_scaled)]
best_n_estimators = np.argmin(errors) + 1
gbc_best = GradientBoostingClassifier(max_depth= best_depth, n_estimators= best_n_estimators)
gbc_best.fit(X_train_scaled, y_train)

y_pred = gbc_best.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((gbc_best.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print('F1-score: ', f1)

# XGBoost (optimized implementation of Gradient Boosting)
import xgboost
from xgboost.callback import EarlyStopping

early_stopping = EarlyStopping(
    rounds= 10,
)

xgb_clf = xgboost.XGBClassifier()
xgb_clf.fit(X_train_scaled, y_train,
            eval_set= [(X_test_scaled, y_test)])

y_pred = xgb_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((xgb_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-score: ", f1)

"""## Voting Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression(max_iter= 5000, class_weight= 'balanced', random_state= 42) # Let's check without logistic classifier
rnd_clf = RandomForestClassifier(n_estimators= 25, class_weight= 'balanced', random_state= 42)
voting_clf = VotingClassifier(
    estimators= [('svm', svm_clf), ('lr', log_clf), ('rf', rnd_clf), ('ac', ada_clf), ('xgb', xgb_clf)]
)
voting_clf.fit(X_train_scaled, y_train)

y_pred = voting_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((gbc_best.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print('F1-score: ', f1)

"""## Stacking"""

from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression(max_iter= 5000, solver= 'lbfgs')
log_clf.fit(X_train_scaled, y_train)

y_pred = log_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((log_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print("F1-score: ", f1)

from sklearn.ensemble import StackingClassifier

estimators = [('lc', log_clf), ('ac', ada_clf), ('xgbc', xgb_clf)]
final_estimator = svm_clf

stk_clf = StackingClassifier(estimators= estimators,
                             final_estimator= final_estimator)
stk_clf.fit(X_train_scaled, y_train)

y_pred = stk_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average= 'macro')
results.append((stk_clf.__class__.__name__, accuracy, f1))
print("Accuracy: ", accuracy)
print('F1-score: ', f1)

"""## Dimensionality Reduction"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.decomposition import PCA

pca = PCA(n_components= 0.95)
X_pca = pca.fit_transform(X_scaled)

classes = np.unique(y)
colors = ['gold', 'green', 'brown', 'red', 'blue']  # or any palette

plt.figure(figsize=(8,6))
for i, cls in enumerate(classes):
    plt.scatter(
        X_pca[y == cls, 0],  # PC1
        X_pca[y == cls, 1],  # PC2
        label= le.classes_[i],      # replace with English/Turkish names if desired
        color=colors[i],
        alpha=0.7,
        edgecolor='k'
    )

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Honey Samples')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""# Representative Signals"""

import pandas as pd
import matplotlib.pyplot as plt


samples_per_class = X.groupby(y).first()  # first row for each class

# Plot
plt.figure(figsize=(10,6))

for class_label, row in samples_per_class.iterrows():
    plt.plot(X.columns.astype(float), row.values, label=f"Class {le.classes_[class_label]}")

plt.xlabel("Wavenumber (cm⁻¹)")
plt.ylabel("Normalized Intensity")
plt.title("Representative FTIR Spectra - One Sample per Class")
plt.gca()  # if spectra are 4000 → 400
plt.legend()
plt.tight_layout()
plt.show()

"""### CNN on reduced dataset"""

import keras
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout

# Define the input shape based on the image
input_shape = (10, 1)

# Create the Sequential model
model = Sequential()

# Add the first Conv1D and MaxPooling1D layers
# The image shows a Conv1D layer with 32 filters, followed by a MaxPooling1D layer
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))

# Add the second Conv1D and MaxPooling1D layers
# The image shows a second Conv1D layer with 32 filters, followed by a MaxPooling1D layer
model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))

# Flatten the output of the convolutional layers
# The image shows the flatten layer with an output shape of (64)
model.add(Flatten())

# Add the first dense layer with a dropout layer
# The image shows the first dense layer with an output shape of (64)
model.add(Dense(64, activation='relu'))
# The image shows a dropout layer with an output shape of (64), we will use a common dropout rate.
model.add(Dropout(0.5))

# Add the final output dense layer
# The image shows the final dense layer with an output shape of (5)
# This suggests a classification task with 5 classes, so 'softmax' is a suitable activation function.
model.add(Dense(5, activation='softmax'))

# Print the model summary to verify the architecture
model.summary()

# Save the model in HDF5 format so it can be viewed in Netron
model.save('model.h5')

!pip install netron

import netron

# Save your model
model.save("cnn_model_on_reduced.h5")

# Start Netron server – it will print the URL
netron.start("cnn_model_on_reduced.h5", browse=True)

from google.colab import files
files.download('cnn_model_on_reduced.h5')

